{"ast":null,"code":"\"use strict\";\n\n/*!\n * Copyright 2022 Google LLC. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\nvar __classPrivateFieldGet = this && this.__classPrivateFieldGet || function (receiver, state, kind, f) {\n  if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n  if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n  return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar _XMLMultiPartUploadHelper_instances, _XMLMultiPartUploadHelper_handleErrorResponse;\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.TransferManager = exports.MultiPartUploadError = void 0;\nconst pLimit = require(\"p-limit\");\nconst path = require(\"path\");\nconst extend = require(\"extend\");\nconst fs_1 = require(\"fs\");\nconst crc32c_1 = require(\"./crc32c\");\nconst google_auth_library_1 = require(\"google-auth-library\");\nconst fast_xml_parser_1 = require(\"fast-xml-parser\");\nconst retry = require(\"async-retry\");\nconst crypto_1 = require(\"crypto\");\n/**\n * Default number of concurrently executing promises to use when calling uploadManyFiles.\n * @experimental\n */\nconst DEFAULT_PARALLEL_UPLOAD_LIMIT = 2;\n/**\n * Default number of concurrently executing promises to use when calling downloadManyFiles.\n * @experimental\n */\nconst DEFAULT_PARALLEL_DOWNLOAD_LIMIT = 2;\n/**\n * Default number of concurrently executing promises to use when calling downloadFileInChunks.\n * @experimental\n */\nconst DEFAULT_PARALLEL_CHUNKED_DOWNLOAD_LIMIT = 2;\n/**\n * The minimum size threshold in bytes at which to apply a chunked download strategy when calling downloadFileInChunks.\n * @experimental\n */\nconst DOWNLOAD_IN_CHUNKS_FILE_SIZE_THRESHOLD = 32 * 1024 * 1024;\n/**\n * The chunk size in bytes to use when calling downloadFileInChunks.\n * @experimental\n */\nconst DOWNLOAD_IN_CHUNKS_DEFAULT_CHUNK_SIZE = 10 * 1024 * 1024;\n/**\n * The chunk size in bytes to use when calling uploadFileInChunks.\n * @experimental\n */\nconst UPLOAD_IN_CHUNKS_DEFAULT_CHUNK_SIZE = 32 * 1024 * 1024;\n/**\n * Default number of concurrently executing promises to use when calling uploadFileInChunks.\n * @experimental\n */\nconst DEFAULT_PARALLEL_CHUNKED_UPLOAD_LIMIT = 2;\nconst EMPTY_REGEX = '(?:)';\nconst defaultMultiPartGenerator = (bucket, fileName, uploadId, partsMap) => {\n  return new XMLMultiPartUploadHelper(bucket, fileName, uploadId, partsMap);\n};\nclass MultiPartUploadError extends Error {\n  constructor(message, uploadId, partsMap) {\n    super(message);\n    this.uploadId = uploadId;\n    this.partsMap = partsMap;\n  }\n}\nexports.MultiPartUploadError = MultiPartUploadError;\n/**\n * Class representing an implementation of MPU in the XML API. This class is not meant for public usage.\n *\n * @private\n * @experimental\n */\nclass XMLMultiPartUploadHelper {\n  constructor(bucket, fileName, uploadId, partsMap) {\n    _XMLMultiPartUploadHelper_instances.add(this);\n    this.authClient = bucket.storage.authClient || new google_auth_library_1.GoogleAuth();\n    this.uploadId = uploadId || '';\n    this.bucket = bucket;\n    this.fileName = fileName;\n    // eslint-disable-next-line prettier/prettier\n    this.baseUrl = `https://${bucket.name}.${new URL(this.bucket.storage.apiEndpoint).hostname}/${fileName}`;\n    this.xmlBuilder = new fast_xml_parser_1.XMLBuilder({\n      arrayNodeName: 'Part'\n    });\n    this.xmlParser = new fast_xml_parser_1.XMLParser();\n    this.partsMap = partsMap || new Map();\n    this.retryOptions = {\n      retries: this.bucket.storage.retryOptions.maxRetries,\n      factor: this.bucket.storage.retryOptions.retryDelayMultiplier,\n      maxTimeout: this.bucket.storage.retryOptions.maxRetryDelay * 1000,\n      maxRetryTime: this.bucket.storage.retryOptions.totalTimeout * 1000\n    };\n  }\n  /**\n   * Initiates a multipart upload (MPU) to the XML API and stores the resultant upload id.\n   *\n   * @returns {Promise<void>}\n   */\n  async initiateUpload() {\n    const url = `${this.baseUrl}?uploads`;\n    return retry(async bail => {\n      try {\n        const res = await this.authClient.request({\n          method: 'POST',\n          url\n        });\n        if (res.data && res.data.error) {\n          throw res.data.error;\n        }\n        const parsedXML = this.xmlParser.parse(res.data);\n        this.uploadId = parsedXML.InitiateMultipartUploadResult.UploadId;\n      } catch (e) {\n        __classPrivateFieldGet(this, _XMLMultiPartUploadHelper_instances, \"m\", _XMLMultiPartUploadHelper_handleErrorResponse).call(this, e, bail);\n      }\n    }, this.retryOptions);\n  }\n  /**\n   * Uploads the provided chunk of data to the XML API using the previously created upload id.\n   *\n   * @param {number} partNumber the sequence number of this chunk.\n   * @param {Buffer} chunk the chunk of data to be uploaded.\n   * @param {string | false} validation whether or not to include the md5 hash in the headers to cause the server\n   * to validate the chunk was not corrupted.\n   * @returns {Promise<void>}\n   */\n  async uploadPart(partNumber, chunk, validation) {\n    const url = `${this.baseUrl}?partNumber=${partNumber}&uploadId=${this.uploadId}`;\n    let headers = {};\n    if (validation === 'md5') {\n      const hash = (0, crypto_1.createHash)('md5').update(chunk).digest('base64');\n      headers = {\n        'Content-MD5': hash\n      };\n    }\n    return retry(async bail => {\n      try {\n        const res = await this.authClient.request({\n          url,\n          method: 'PUT',\n          body: chunk,\n          headers\n        });\n        if (res.data && res.data.error) {\n          throw res.data.error;\n        }\n        this.partsMap.set(partNumber, res.headers['etag']);\n      } catch (e) {\n        __classPrivateFieldGet(this, _XMLMultiPartUploadHelper_instances, \"m\", _XMLMultiPartUploadHelper_handleErrorResponse).call(this, e, bail);\n      }\n    }, this.retryOptions);\n  }\n  /**\n   * Sends the final request of the MPU to tell GCS the upload is now complete.\n   *\n   * @returns {Promise<void>}\n   */\n  async completeUpload() {\n    const url = `${this.baseUrl}?uploadId=${this.uploadId}`;\n    const sortedMap = new Map([...this.partsMap.entries()].sort((a, b) => a[0] - b[0]));\n    const parts = [];\n    for (const entry of sortedMap.entries()) {\n      parts.push({\n        PartNumber: entry[0],\n        ETag: entry[1]\n      });\n    }\n    const body = `<CompleteMultipartUpload>${this.xmlBuilder.build(parts)}</CompleteMultipartUpload>`;\n    return retry(async bail => {\n      try {\n        const res = await this.authClient.request({\n          url,\n          method: 'POST',\n          body\n        });\n        if (res.data && res.data.error) {\n          throw res.data.error;\n        }\n        return res;\n      } catch (e) {\n        __classPrivateFieldGet(this, _XMLMultiPartUploadHelper_instances, \"m\", _XMLMultiPartUploadHelper_handleErrorResponse).call(this, e, bail);\n        return;\n      }\n    }, this.retryOptions);\n  }\n}\n_XMLMultiPartUploadHelper_instances = new WeakSet(), _XMLMultiPartUploadHelper_handleErrorResponse = function _XMLMultiPartUploadHelper_handleErrorResponse(err, bail) {\n  if (this.bucket.storage.retryOptions.autoRetry && this.bucket.storage.retryOptions.retryableErrorFn(err)) {\n    throw err;\n  } else {\n    bail(err);\n  }\n};\n/**\n * Create a TransferManager object to perform parallel transfer operations on a Cloud Storage bucket.\n *\n * @class\n * @hideconstructor\n *\n * @param {Bucket} bucket A {@link Bucket} instance\n * @experimental\n */\nclass TransferManager {\n  constructor(bucket) {\n    this.bucket = bucket;\n  }\n  /**\n   * @typedef {object} UploadManyFilesOptions\n   * @property {number} [concurrencyLimit] The number of concurrently executing promises\n   * to use when uploading the files.\n   * @property {boolean} [skipIfExists] Do not upload the file if it already exists in\n   * the bucket. This will set the precondition ifGenerationMatch = 0.\n   * @property {string} [prefix] A prefix to append to all of the uploaded files.\n   * @property {object} [passthroughOptions] {@link UploadOptions} Options to be passed through\n   * to each individual upload operation.\n   * @experimental\n   */\n  /**\n   * Upload multiple files in parallel to the bucket. This is a convenience method\n   * that utilizes {@link Bucket#upload} to perform the upload.\n   *\n   * @param {array | string} [filePathsOrDirectory] An array of fully qualified paths to the files or a directory name.\n   * If a directory name is provided, the directory will be recursively walked and all files will be added to the upload list.\n   * to be uploaded to the bucket\n   * @param {UploadManyFilesOptions} [options] Configuration options.\n   * @returns {Promise<UploadResponse[]>}\n   *\n   * @example\n   * ```\n   * const {Storage} = require('@google-cloud/storage');\n   * const storage = new Storage();\n   * const bucket = storage.bucket('my-bucket');\n   * const transferManager = new TransferManager(bucket);\n   *\n   * //-\n   * // Upload multiple files in parallel.\n   * //-\n   * const response = await transferManager.uploadManyFiles(['/local/path/file1.txt, 'local/path/file2.txt']);\n   * // Your bucket now contains:\n   * // - \"local/path/file1.txt\" (with the contents of '/local/path/file1.txt')\n   * // - \"local/path/file2.txt\" (with the contents of '/local/path/file2.txt')\n   * const response = await transferManager.uploadManyFiles('/local/directory');\n   * // Your bucket will now contain all files contained in '/local/directory' maintaining the subdirectory structure.\n   * ```\n   * @experimental\n   */\n  async uploadManyFiles(filePathsOrDirectory, options = {}) {\n    var _a;\n    if (options.skipIfExists && ((_a = options.passthroughOptions) === null || _a === void 0 ? void 0 : _a.preconditionOpts)) {\n      options.passthroughOptions.preconditionOpts.ifGenerationMatch = 0;\n    } else if (options.skipIfExists && options.passthroughOptions === undefined) {\n      options.passthroughOptions = {\n        preconditionOpts: {\n          ifGenerationMatch: 0\n        }\n      };\n    }\n    const limit = pLimit(options.concurrencyLimit || DEFAULT_PARALLEL_UPLOAD_LIMIT);\n    const promises = [];\n    let allPaths = [];\n    if (!Array.isArray(filePathsOrDirectory)) {\n      for await (const curPath of this.getPathsFromDirectory(filePathsOrDirectory)) {\n        allPaths.push(curPath);\n      }\n    } else {\n      allPaths = filePathsOrDirectory;\n    }\n    for (const filePath of allPaths) {\n      const stat = await fs_1.promises.lstat(filePath);\n      if (stat.isDirectory()) {\n        continue;\n      }\n      const passThroughOptionsCopy = extend(true, {}, options.passthroughOptions);\n      passThroughOptionsCopy.destination = filePath;\n      if (options.prefix) {\n        passThroughOptionsCopy.destination = path.join(options.prefix, passThroughOptionsCopy.destination);\n      }\n      promises.push(limit(() => this.bucket.upload(filePath, passThroughOptionsCopy)));\n    }\n    return Promise.all(promises);\n  }\n  /**\n   * @typedef {object} DownloadManyFilesOptions\n   * @property {number} [concurrencyLimit] The number of concurrently executing promises\n   * to use when downloading the files.\n   * @property {string} [prefix] A prefix to append to all of the downloaded files.\n   * @property {string} [stripPrefix] A prefix to remove from all of the downloaded files.\n   * @property {object} [passthroughOptions] {@link DownloadOptions} Options to be passed through\n   * to each individual download operation.\n   * @experimental\n   */\n  /**\n   * Download multiple files in parallel to the local filesystem. This is a convenience method\n   * that utilizes {@link File#download} to perform the download.\n   *\n   * @param {array | string} [filesOrFolder] An array of file name strings or file objects to be downloaded. If\n   * a string is provided this will be treated as a GCS prefix and all files with that prefix will be downloaded.\n   * @param {DownloadManyFilesOptions} [options] Configuration options.\n   * @returns {Promise<DownloadResponse[]>}\n   *\n   * @example\n   * ```\n   * const {Storage} = require('@google-cloud/storage');\n   * const storage = new Storage();\n   * const bucket = storage.bucket('my-bucket');\n   * const transferManager = new TransferManager(bucket);\n   *\n   * //-\n   * // Download multiple files in parallel.\n   * //-\n   * const response = await transferManager.downloadManyFiles(['file1.txt', 'file2.txt']);\n   * // The following files have been downloaded:\n   * // - \"file1.txt\" (with the contents from my-bucket.file1.txt)\n   * // - \"file2.txt\" (with the contents from my-bucket.file2.txt)\n   * const response = await transferManager.downloadManyFiles([bucket.File('file1.txt'), bucket.File('file2.txt')]);\n   * // The following files have been downloaded:\n   * // - \"file1.txt\" (with the contents from my-bucket.file1.txt)\n   * // - \"file2.txt\" (with the contents from my-bucket.file2.txt)\n   * const response = await transferManager.downloadManyFiles('test-folder');\n   * // All files with GCS prefix of 'test-folder' have been downloaded.\n   * ```\n   * @experimental\n   */\n  async downloadManyFiles(filesOrFolder, options = {}) {\n    const limit = pLimit(options.concurrencyLimit || DEFAULT_PARALLEL_DOWNLOAD_LIMIT);\n    const promises = [];\n    let files = [];\n    if (!Array.isArray(filesOrFolder)) {\n      const directoryFiles = await this.bucket.getFiles({\n        prefix: filesOrFolder\n      });\n      files = directoryFiles[0];\n    } else {\n      files = filesOrFolder.map(curFile => {\n        if (typeof curFile === 'string') {\n          return this.bucket.file(curFile);\n        }\n        return curFile;\n      });\n    }\n    const stripRegexString = options.stripPrefix ? `^${options.stripPrefix}` : EMPTY_REGEX;\n    const regex = new RegExp(stripRegexString, 'g');\n    for (const file of files) {\n      const passThroughOptionsCopy = extend(true, {}, options.passthroughOptions);\n      if (options.prefix) {\n        passThroughOptionsCopy.destination = path.join(options.prefix || '', passThroughOptionsCopy.destination || '', file.name);\n      }\n      if (options.stripPrefix) {\n        passThroughOptionsCopy.destination = file.name.replace(regex, '');\n      }\n      promises.push(limit(() => file.download(passThroughOptionsCopy)));\n    }\n    return Promise.all(promises);\n  }\n  /**\n   * @typedef {object} DownloadFileInChunksOptions\n   * @property {number} [concurrencyLimit] The number of concurrently executing promises\n   * to use when downloading the file.\n   * @property {number} [chunkSizeBytes] The size in bytes of each chunk to be downloaded.\n   * @property {string | boolean} [validation] Whether or not to perform a CRC32C validation check when download is complete.\n   * @experimental\n   */\n  /**\n   * Download a large file in chunks utilizing parallel download operations. This is a convenience method\n   * that utilizes {@link File#download} to perform the download.\n   *\n   * @param {object} [file | string] {@link File} to download.\n   * @param {DownloadFileInChunksOptions} [options] Configuration options.\n   * @returns {Promise<DownloadResponse>}\n   *\n   * @example\n   * ```\n   * const {Storage} = require('@google-cloud/storage');\n   * const storage = new Storage();\n   * const bucket = storage.bucket('my-bucket');\n   * const transferManager = new TransferManager(bucket);\n   *\n   * //-\n   * // Download a large file in chunks utilizing parallel operations.\n   * //-\n   * const response = await transferManager.downloadFileInChunks(bucket.file('large-file.txt');\n   * // Your local directory now contains:\n   * // - \"large-file.txt\" (with the contents from my-bucket.large-file.txt)\n   * ```\n   * @experimental\n   */\n  async downloadFileInChunks(fileOrName, options = {}) {\n    let chunkSize = options.chunkSizeBytes || DOWNLOAD_IN_CHUNKS_DEFAULT_CHUNK_SIZE;\n    let limit = pLimit(options.concurrencyLimit || DEFAULT_PARALLEL_CHUNKED_DOWNLOAD_LIMIT);\n    const promises = [];\n    const file = typeof fileOrName === 'string' ? this.bucket.file(fileOrName) : fileOrName;\n    const fileInfo = await file.get();\n    const size = parseInt(fileInfo[0].metadata.size);\n    // If the file size does not meet the threshold download it as a single chunk.\n    if (size < DOWNLOAD_IN_CHUNKS_FILE_SIZE_THRESHOLD) {\n      limit = pLimit(1);\n      chunkSize = size;\n    }\n    let start = 0;\n    const filePath = options.destination || path.basename(file.name);\n    const fileToWrite = await fs_1.promises.open(filePath, 'w+');\n    while (start < size) {\n      const chunkStart = start;\n      let chunkEnd = start + chunkSize - 1;\n      chunkEnd = chunkEnd > size ? size : chunkEnd;\n      promises.push(limit(() => file.download({\n        start: chunkStart,\n        end: chunkEnd\n      }).then(resp => {\n        return fileToWrite.write(resp[0], 0, resp[0].length, chunkStart);\n      })));\n      start += chunkSize;\n    }\n    return new Promise((resolve, reject) => {\n      let results;\n      Promise.all(promises).then(data => {\n        results = data.map(result => result.buffer);\n        if (options.validation === 'crc32c') {\n          return crc32c_1.CRC32C.fromFile(filePath);\n        }\n        return;\n      }).then(() => {\n        resolve(results);\n      }).catch(e => {\n        reject(e);\n      }).finally(() => {\n        fileToWrite.close();\n      });\n    });\n  }\n  /**\n   * @typedef {object} UploadFileInChunksOptions\n   * @property {number} [concurrencyLimit] The number of concurrently executing promises\n   * to use when uploading the file.\n   * @property {number} [chunkSizeBytes] The size in bytes of each chunk to be uploaded.\n   * @property {string} [uploadName] Name of the file when saving to GCS. If ommitted the name is taken from the file path.\n   * @property {number} [maxQueueSize] The number of chunks to be uploaded to hold in memory concurrently. If not specified\n   * defaults to the specified concurrency limit.\n   * @property {string} [uploadId] If specified attempts to resume a previous upload.\n   * @property {Map} [partsMap] If specified alongside uploadId, attempts to resume a previous upload from the last chunk\n   * specified in partsMap\n   * @experimental\n   */\n  /**\n   * Upload a large file in chunks utilizing parallel upload opertions. If the upload fails, an uploadId and\n   * map containing all the successfully uploaded parts will be returned to the caller. These arguments can be used to\n   * resume the upload.\n   *\n   * @param {string} [filePath] The path of the file to be uploaded\n   * @param {UploadFileInChunksOptions} [options] Configuration options.\n   * @param {MultiPartHelperGenerator} [generator] A function that will return a type that implements the MPU interface. Most users will not need to use this.\n   * @returns {Promise<void>} If successful a promise resolving to void, otherwise a error containing the message, uploadid, and parts map.\n   *\n   * @example\n   * ```\n   * const {Storage} = require('@google-cloud/storage');\n   * const storage = new Storage();\n   * const bucket = storage.bucket('my-bucket');\n   * const transferManager = new TransferManager(bucket);\n   *\n   * //-\n   * // Upload a large file in chunks utilizing parallel operations.\n   * //-\n   * const response = await transferManager.uploadFileInChunks('large-file.txt');\n   * // Your bucket now contains:\n   * // - \"large-file.txt\"\n   * ```\n   *\n   * @experimental\n   */\n  async uploadFileInChunks(filePath, options = {}, generator = defaultMultiPartGenerator) {\n    const chunkSize = options.chunkSizeBytes || UPLOAD_IN_CHUNKS_DEFAULT_CHUNK_SIZE;\n    const limit = pLimit(options.concurrencyLimit || DEFAULT_PARALLEL_CHUNKED_UPLOAD_LIMIT);\n    const maxQueueSize = options.maxQueueSize || options.concurrencyLimit || DEFAULT_PARALLEL_CHUNKED_UPLOAD_LIMIT;\n    const fileName = options.uploadName || path.basename(filePath);\n    const mpuHelper = generator(this.bucket, fileName, options.uploadId, options.partsMap);\n    let partNumber = 1;\n    let promises = [];\n    try {\n      if (options.uploadId === undefined) {\n        await mpuHelper.initiateUpload();\n      }\n      const startOrResumptionByte = mpuHelper.partsMap.size * chunkSize;\n      const readStream = (0, fs_1.createReadStream)(filePath, {\n        highWaterMark: chunkSize,\n        start: startOrResumptionByte\n      });\n      // p-limit only limits the number of running promises. We do not want to hold an entire\n      // large file in memory at once so promises acts a queue that will hold only maxQueueSize in memory.\n      for await (const curChunk of readStream) {\n        if (promises.length >= maxQueueSize) {\n          await Promise.all(promises);\n          promises = [];\n        }\n        promises.push(limit(() => mpuHelper.uploadPart(partNumber++, curChunk, options.validation)));\n      }\n      await Promise.all(promises);\n      return await mpuHelper.completeUpload();\n    } catch (e) {\n      throw new MultiPartUploadError(e.message, mpuHelper.uploadId, mpuHelper.partsMap);\n    }\n  }\n  async *getPathsFromDirectory(directory) {\n    const filesAndSubdirectories = await fs_1.promises.readdir(directory, {\n      withFileTypes: true\n    });\n    for (const curFileOrDirectory of filesAndSubdirectories) {\n      const fullPath = path.join(directory, curFileOrDirectory.name);\n      curFileOrDirectory.isDirectory() ? yield* this.getPathsFromDirectory(fullPath) : yield fullPath;\n    }\n  }\n}\nexports.TransferManager = TransferManager;","map":{"version":3,"names":["__classPrivateFieldGet","receiver","state","kind","f","TypeError","has","call","value","get","_XMLMultiPartUploadHelper_instances","_XMLMultiPartUploadHelper_handleErrorResponse","Object","defineProperty","exports","TransferManager","MultiPartUploadError","pLimit","require","path","extend","fs_1","crc32c_1","google_auth_library_1","fast_xml_parser_1","retry","crypto_1","DEFAULT_PARALLEL_UPLOAD_LIMIT","DEFAULT_PARALLEL_DOWNLOAD_LIMIT","DEFAULT_PARALLEL_CHUNKED_DOWNLOAD_LIMIT","DOWNLOAD_IN_CHUNKS_FILE_SIZE_THRESHOLD","DOWNLOAD_IN_CHUNKS_DEFAULT_CHUNK_SIZE","UPLOAD_IN_CHUNKS_DEFAULT_CHUNK_SIZE","DEFAULT_PARALLEL_CHUNKED_UPLOAD_LIMIT","EMPTY_REGEX","defaultMultiPartGenerator","bucket","fileName","uploadId","partsMap","XMLMultiPartUploadHelper","Error","constructor","message","add","authClient","storage","GoogleAuth","baseUrl","name","URL","apiEndpoint","hostname","xmlBuilder","XMLBuilder","arrayNodeName","xmlParser","XMLParser","Map","retryOptions","retries","maxRetries","factor","retryDelayMultiplier","maxTimeout","maxRetryDelay","maxRetryTime","totalTimeout","initiateUpload","url","bail","res","request","method","data","error","parsedXML","parse","InitiateMultipartUploadResult","UploadId","e","uploadPart","partNumber","chunk","validation","headers","hash","createHash","update","digest","body","set","completeUpload","sortedMap","entries","sort","a","b","parts","entry","push","PartNumber","ETag","build","WeakSet","err","autoRetry","retryableErrorFn","uploadManyFiles","filePathsOrDirectory","options","_a","skipIfExists","passthroughOptions","preconditionOpts","ifGenerationMatch","undefined","limit","concurrencyLimit","promises","allPaths","Array","isArray","curPath","getPathsFromDirectory","filePath","stat","lstat","isDirectory","passThroughOptionsCopy","destination","prefix","join","upload","Promise","all","downloadManyFiles","filesOrFolder","files","directoryFiles","getFiles","map","curFile","file","stripRegexString","stripPrefix","regex","RegExp","replace","download","downloadFileInChunks","fileOrName","chunkSize","chunkSizeBytes","fileInfo","size","parseInt","metadata","start","basename","fileToWrite","open","chunkStart","chunkEnd","end","then","resp","write","length","resolve","reject","results","result","buffer","CRC32C","fromFile","catch","finally","close","uploadFileInChunks","generator","maxQueueSize","uploadName","mpuHelper","startOrResumptionByte","readStream","createReadStream","highWaterMark","curChunk","directory","filesAndSubdirectories","readdir","withFileTypes","curFileOrDirectory","fullPath"],"sources":["/Users/charlpro/scoreboard/node_modules/@google-cloud/storage/build/src/transfer-manager.js"],"sourcesContent":["\"use strict\";\n/*!\n * Copyright 2022 Google LLC. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\nvar __classPrivateFieldGet = (this && this.__classPrivateFieldGet) || function (receiver, state, kind, f) {\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar _XMLMultiPartUploadHelper_instances, _XMLMultiPartUploadHelper_handleErrorResponse;\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.TransferManager = exports.MultiPartUploadError = void 0;\nconst pLimit = require(\"p-limit\");\nconst path = require(\"path\");\nconst extend = require(\"extend\");\nconst fs_1 = require(\"fs\");\nconst crc32c_1 = require(\"./crc32c\");\nconst google_auth_library_1 = require(\"google-auth-library\");\nconst fast_xml_parser_1 = require(\"fast-xml-parser\");\nconst retry = require(\"async-retry\");\nconst crypto_1 = require(\"crypto\");\n/**\n * Default number of concurrently executing promises to use when calling uploadManyFiles.\n * @experimental\n */\nconst DEFAULT_PARALLEL_UPLOAD_LIMIT = 2;\n/**\n * Default number of concurrently executing promises to use when calling downloadManyFiles.\n * @experimental\n */\nconst DEFAULT_PARALLEL_DOWNLOAD_LIMIT = 2;\n/**\n * Default number of concurrently executing promises to use when calling downloadFileInChunks.\n * @experimental\n */\nconst DEFAULT_PARALLEL_CHUNKED_DOWNLOAD_LIMIT = 2;\n/**\n * The minimum size threshold in bytes at which to apply a chunked download strategy when calling downloadFileInChunks.\n * @experimental\n */\nconst DOWNLOAD_IN_CHUNKS_FILE_SIZE_THRESHOLD = 32 * 1024 * 1024;\n/**\n * The chunk size in bytes to use when calling downloadFileInChunks.\n * @experimental\n */\nconst DOWNLOAD_IN_CHUNKS_DEFAULT_CHUNK_SIZE = 10 * 1024 * 1024;\n/**\n * The chunk size in bytes to use when calling uploadFileInChunks.\n * @experimental\n */\nconst UPLOAD_IN_CHUNKS_DEFAULT_CHUNK_SIZE = 32 * 1024 * 1024;\n/**\n * Default number of concurrently executing promises to use when calling uploadFileInChunks.\n * @experimental\n */\nconst DEFAULT_PARALLEL_CHUNKED_UPLOAD_LIMIT = 2;\nconst EMPTY_REGEX = '(?:)';\nconst defaultMultiPartGenerator = (bucket, fileName, uploadId, partsMap) => {\n    return new XMLMultiPartUploadHelper(bucket, fileName, uploadId, partsMap);\n};\nclass MultiPartUploadError extends Error {\n    constructor(message, uploadId, partsMap) {\n        super(message);\n        this.uploadId = uploadId;\n        this.partsMap = partsMap;\n    }\n}\nexports.MultiPartUploadError = MultiPartUploadError;\n/**\n * Class representing an implementation of MPU in the XML API. This class is not meant for public usage.\n *\n * @private\n * @experimental\n */\nclass XMLMultiPartUploadHelper {\n    constructor(bucket, fileName, uploadId, partsMap) {\n        _XMLMultiPartUploadHelper_instances.add(this);\n        this.authClient = bucket.storage.authClient || new google_auth_library_1.GoogleAuth();\n        this.uploadId = uploadId || '';\n        this.bucket = bucket;\n        this.fileName = fileName;\n        // eslint-disable-next-line prettier/prettier\n        this.baseUrl = `https://${bucket.name}.${new URL(this.bucket.storage.apiEndpoint).hostname}/${fileName}`;\n        this.xmlBuilder = new fast_xml_parser_1.XMLBuilder({ arrayNodeName: 'Part' });\n        this.xmlParser = new fast_xml_parser_1.XMLParser();\n        this.partsMap = partsMap || new Map();\n        this.retryOptions = {\n            retries: this.bucket.storage.retryOptions.maxRetries,\n            factor: this.bucket.storage.retryOptions.retryDelayMultiplier,\n            maxTimeout: this.bucket.storage.retryOptions.maxRetryDelay * 1000,\n            maxRetryTime: this.bucket.storage.retryOptions.totalTimeout * 1000,\n        };\n    }\n    /**\n     * Initiates a multipart upload (MPU) to the XML API and stores the resultant upload id.\n     *\n     * @returns {Promise<void>}\n     */\n    async initiateUpload() {\n        const url = `${this.baseUrl}?uploads`;\n        return retry(async (bail) => {\n            try {\n                const res = await this.authClient.request({\n                    method: 'POST',\n                    url,\n                });\n                if (res.data && res.data.error) {\n                    throw res.data.error;\n                }\n                const parsedXML = this.xmlParser.parse(res.data);\n                this.uploadId = parsedXML.InitiateMultipartUploadResult.UploadId;\n            }\n            catch (e) {\n                __classPrivateFieldGet(this, _XMLMultiPartUploadHelper_instances, \"m\", _XMLMultiPartUploadHelper_handleErrorResponse).call(this, e, bail);\n            }\n        }, this.retryOptions);\n    }\n    /**\n     * Uploads the provided chunk of data to the XML API using the previously created upload id.\n     *\n     * @param {number} partNumber the sequence number of this chunk.\n     * @param {Buffer} chunk the chunk of data to be uploaded.\n     * @param {string | false} validation whether or not to include the md5 hash in the headers to cause the server\n     * to validate the chunk was not corrupted.\n     * @returns {Promise<void>}\n     */\n    async uploadPart(partNumber, chunk, validation) {\n        const url = `${this.baseUrl}?partNumber=${partNumber}&uploadId=${this.uploadId}`;\n        let headers = {};\n        if (validation === 'md5') {\n            const hash = (0, crypto_1.createHash)('md5').update(chunk).digest('base64');\n            headers = {\n                'Content-MD5': hash,\n            };\n        }\n        return retry(async (bail) => {\n            try {\n                const res = await this.authClient.request({\n                    url,\n                    method: 'PUT',\n                    body: chunk,\n                    headers,\n                });\n                if (res.data && res.data.error) {\n                    throw res.data.error;\n                }\n                this.partsMap.set(partNumber, res.headers['etag']);\n            }\n            catch (e) {\n                __classPrivateFieldGet(this, _XMLMultiPartUploadHelper_instances, \"m\", _XMLMultiPartUploadHelper_handleErrorResponse).call(this, e, bail);\n            }\n        }, this.retryOptions);\n    }\n    /**\n     * Sends the final request of the MPU to tell GCS the upload is now complete.\n     *\n     * @returns {Promise<void>}\n     */\n    async completeUpload() {\n        const url = `${this.baseUrl}?uploadId=${this.uploadId}`;\n        const sortedMap = new Map([...this.partsMap.entries()].sort((a, b) => a[0] - b[0]));\n        const parts = [];\n        for (const entry of sortedMap.entries()) {\n            parts.push({ PartNumber: entry[0], ETag: entry[1] });\n        }\n        const body = `<CompleteMultipartUpload>${this.xmlBuilder.build(parts)}</CompleteMultipartUpload>`;\n        return retry(async (bail) => {\n            try {\n                const res = await this.authClient.request({\n                    url,\n                    method: 'POST',\n                    body,\n                });\n                if (res.data && res.data.error) {\n                    throw res.data.error;\n                }\n                return res;\n            }\n            catch (e) {\n                __classPrivateFieldGet(this, _XMLMultiPartUploadHelper_instances, \"m\", _XMLMultiPartUploadHelper_handleErrorResponse).call(this, e, bail);\n                return;\n            }\n        }, this.retryOptions);\n    }\n}\n_XMLMultiPartUploadHelper_instances = new WeakSet(), _XMLMultiPartUploadHelper_handleErrorResponse = function _XMLMultiPartUploadHelper_handleErrorResponse(err, bail) {\n    if (this.bucket.storage.retryOptions.autoRetry &&\n        this.bucket.storage.retryOptions.retryableErrorFn(err)) {\n        throw err;\n    }\n    else {\n        bail(err);\n    }\n};\n/**\n * Create a TransferManager object to perform parallel transfer operations on a Cloud Storage bucket.\n *\n * @class\n * @hideconstructor\n *\n * @param {Bucket} bucket A {@link Bucket} instance\n * @experimental\n */\nclass TransferManager {\n    constructor(bucket) {\n        this.bucket = bucket;\n    }\n    /**\n     * @typedef {object} UploadManyFilesOptions\n     * @property {number} [concurrencyLimit] The number of concurrently executing promises\n     * to use when uploading the files.\n     * @property {boolean} [skipIfExists] Do not upload the file if it already exists in\n     * the bucket. This will set the precondition ifGenerationMatch = 0.\n     * @property {string} [prefix] A prefix to append to all of the uploaded files.\n     * @property {object} [passthroughOptions] {@link UploadOptions} Options to be passed through\n     * to each individual upload operation.\n     * @experimental\n     */\n    /**\n     * Upload multiple files in parallel to the bucket. This is a convenience method\n     * that utilizes {@link Bucket#upload} to perform the upload.\n     *\n     * @param {array | string} [filePathsOrDirectory] An array of fully qualified paths to the files or a directory name.\n     * If a directory name is provided, the directory will be recursively walked and all files will be added to the upload list.\n     * to be uploaded to the bucket\n     * @param {UploadManyFilesOptions} [options] Configuration options.\n     * @returns {Promise<UploadResponse[]>}\n     *\n     * @example\n     * ```\n     * const {Storage} = require('@google-cloud/storage');\n     * const storage = new Storage();\n     * const bucket = storage.bucket('my-bucket');\n     * const transferManager = new TransferManager(bucket);\n     *\n     * //-\n     * // Upload multiple files in parallel.\n     * //-\n     * const response = await transferManager.uploadManyFiles(['/local/path/file1.txt, 'local/path/file2.txt']);\n     * // Your bucket now contains:\n     * // - \"local/path/file1.txt\" (with the contents of '/local/path/file1.txt')\n     * // - \"local/path/file2.txt\" (with the contents of '/local/path/file2.txt')\n     * const response = await transferManager.uploadManyFiles('/local/directory');\n     * // Your bucket will now contain all files contained in '/local/directory' maintaining the subdirectory structure.\n     * ```\n     * @experimental\n     */\n    async uploadManyFiles(filePathsOrDirectory, options = {}) {\n        var _a;\n        if (options.skipIfExists && ((_a = options.passthroughOptions) === null || _a === void 0 ? void 0 : _a.preconditionOpts)) {\n            options.passthroughOptions.preconditionOpts.ifGenerationMatch = 0;\n        }\n        else if (options.skipIfExists &&\n            options.passthroughOptions === undefined) {\n            options.passthroughOptions = {\n                preconditionOpts: {\n                    ifGenerationMatch: 0,\n                },\n            };\n        }\n        const limit = pLimit(options.concurrencyLimit || DEFAULT_PARALLEL_UPLOAD_LIMIT);\n        const promises = [];\n        let allPaths = [];\n        if (!Array.isArray(filePathsOrDirectory)) {\n            for await (const curPath of this.getPathsFromDirectory(filePathsOrDirectory)) {\n                allPaths.push(curPath);\n            }\n        }\n        else {\n            allPaths = filePathsOrDirectory;\n        }\n        for (const filePath of allPaths) {\n            const stat = await fs_1.promises.lstat(filePath);\n            if (stat.isDirectory()) {\n                continue;\n            }\n            const passThroughOptionsCopy = extend(true, {}, options.passthroughOptions);\n            passThroughOptionsCopy.destination = filePath;\n            if (options.prefix) {\n                passThroughOptionsCopy.destination = path.join(options.prefix, passThroughOptionsCopy.destination);\n            }\n            promises.push(limit(() => this.bucket.upload(filePath, passThroughOptionsCopy)));\n        }\n        return Promise.all(promises);\n    }\n    /**\n     * @typedef {object} DownloadManyFilesOptions\n     * @property {number} [concurrencyLimit] The number of concurrently executing promises\n     * to use when downloading the files.\n     * @property {string} [prefix] A prefix to append to all of the downloaded files.\n     * @property {string} [stripPrefix] A prefix to remove from all of the downloaded files.\n     * @property {object} [passthroughOptions] {@link DownloadOptions} Options to be passed through\n     * to each individual download operation.\n     * @experimental\n     */\n    /**\n     * Download multiple files in parallel to the local filesystem. This is a convenience method\n     * that utilizes {@link File#download} to perform the download.\n     *\n     * @param {array | string} [filesOrFolder] An array of file name strings or file objects to be downloaded. If\n     * a string is provided this will be treated as a GCS prefix and all files with that prefix will be downloaded.\n     * @param {DownloadManyFilesOptions} [options] Configuration options.\n     * @returns {Promise<DownloadResponse[]>}\n     *\n     * @example\n     * ```\n     * const {Storage} = require('@google-cloud/storage');\n     * const storage = new Storage();\n     * const bucket = storage.bucket('my-bucket');\n     * const transferManager = new TransferManager(bucket);\n     *\n     * //-\n     * // Download multiple files in parallel.\n     * //-\n     * const response = await transferManager.downloadManyFiles(['file1.txt', 'file2.txt']);\n     * // The following files have been downloaded:\n     * // - \"file1.txt\" (with the contents from my-bucket.file1.txt)\n     * // - \"file2.txt\" (with the contents from my-bucket.file2.txt)\n     * const response = await transferManager.downloadManyFiles([bucket.File('file1.txt'), bucket.File('file2.txt')]);\n     * // The following files have been downloaded:\n     * // - \"file1.txt\" (with the contents from my-bucket.file1.txt)\n     * // - \"file2.txt\" (with the contents from my-bucket.file2.txt)\n     * const response = await transferManager.downloadManyFiles('test-folder');\n     * // All files with GCS prefix of 'test-folder' have been downloaded.\n     * ```\n     * @experimental\n     */\n    async downloadManyFiles(filesOrFolder, options = {}) {\n        const limit = pLimit(options.concurrencyLimit || DEFAULT_PARALLEL_DOWNLOAD_LIMIT);\n        const promises = [];\n        let files = [];\n        if (!Array.isArray(filesOrFolder)) {\n            const directoryFiles = await this.bucket.getFiles({\n                prefix: filesOrFolder,\n            });\n            files = directoryFiles[0];\n        }\n        else {\n            files = filesOrFolder.map(curFile => {\n                if (typeof curFile === 'string') {\n                    return this.bucket.file(curFile);\n                }\n                return curFile;\n            });\n        }\n        const stripRegexString = options.stripPrefix\n            ? `^${options.stripPrefix}`\n            : EMPTY_REGEX;\n        const regex = new RegExp(stripRegexString, 'g');\n        for (const file of files) {\n            const passThroughOptionsCopy = extend(true, {}, options.passthroughOptions);\n            if (options.prefix) {\n                passThroughOptionsCopy.destination = path.join(options.prefix || '', passThroughOptionsCopy.destination || '', file.name);\n            }\n            if (options.stripPrefix) {\n                passThroughOptionsCopy.destination = file.name.replace(regex, '');\n            }\n            promises.push(limit(() => file.download(passThroughOptionsCopy)));\n        }\n        return Promise.all(promises);\n    }\n    /**\n     * @typedef {object} DownloadFileInChunksOptions\n     * @property {number} [concurrencyLimit] The number of concurrently executing promises\n     * to use when downloading the file.\n     * @property {number} [chunkSizeBytes] The size in bytes of each chunk to be downloaded.\n     * @property {string | boolean} [validation] Whether or not to perform a CRC32C validation check when download is complete.\n     * @experimental\n     */\n    /**\n     * Download a large file in chunks utilizing parallel download operations. This is a convenience method\n     * that utilizes {@link File#download} to perform the download.\n     *\n     * @param {object} [file | string] {@link File} to download.\n     * @param {DownloadFileInChunksOptions} [options] Configuration options.\n     * @returns {Promise<DownloadResponse>}\n     *\n     * @example\n     * ```\n     * const {Storage} = require('@google-cloud/storage');\n     * const storage = new Storage();\n     * const bucket = storage.bucket('my-bucket');\n     * const transferManager = new TransferManager(bucket);\n     *\n     * //-\n     * // Download a large file in chunks utilizing parallel operations.\n     * //-\n     * const response = await transferManager.downloadFileInChunks(bucket.file('large-file.txt');\n     * // Your local directory now contains:\n     * // - \"large-file.txt\" (with the contents from my-bucket.large-file.txt)\n     * ```\n     * @experimental\n     */\n    async downloadFileInChunks(fileOrName, options = {}) {\n        let chunkSize = options.chunkSizeBytes || DOWNLOAD_IN_CHUNKS_DEFAULT_CHUNK_SIZE;\n        let limit = pLimit(options.concurrencyLimit || DEFAULT_PARALLEL_CHUNKED_DOWNLOAD_LIMIT);\n        const promises = [];\n        const file = typeof fileOrName === 'string'\n            ? this.bucket.file(fileOrName)\n            : fileOrName;\n        const fileInfo = await file.get();\n        const size = parseInt(fileInfo[0].metadata.size);\n        // If the file size does not meet the threshold download it as a single chunk.\n        if (size < DOWNLOAD_IN_CHUNKS_FILE_SIZE_THRESHOLD) {\n            limit = pLimit(1);\n            chunkSize = size;\n        }\n        let start = 0;\n        const filePath = options.destination || path.basename(file.name);\n        const fileToWrite = await fs_1.promises.open(filePath, 'w+');\n        while (start < size) {\n            const chunkStart = start;\n            let chunkEnd = start + chunkSize - 1;\n            chunkEnd = chunkEnd > size ? size : chunkEnd;\n            promises.push(limit(() => file.download({ start: chunkStart, end: chunkEnd }).then(resp => {\n                return fileToWrite.write(resp[0], 0, resp[0].length, chunkStart);\n            })));\n            start += chunkSize;\n        }\n        return new Promise((resolve, reject) => {\n            let results;\n            Promise.all(promises)\n                .then(data => {\n                results = data.map(result => result.buffer);\n                if (options.validation === 'crc32c') {\n                    return crc32c_1.CRC32C.fromFile(filePath);\n                }\n                return;\n            })\n                .then(() => {\n                resolve(results);\n            })\n                .catch(e => {\n                reject(e);\n            })\n                .finally(() => {\n                fileToWrite.close();\n            });\n        });\n    }\n    /**\n     * @typedef {object} UploadFileInChunksOptions\n     * @property {number} [concurrencyLimit] The number of concurrently executing promises\n     * to use when uploading the file.\n     * @property {number} [chunkSizeBytes] The size in bytes of each chunk to be uploaded.\n     * @property {string} [uploadName] Name of the file when saving to GCS. If ommitted the name is taken from the file path.\n     * @property {number} [maxQueueSize] The number of chunks to be uploaded to hold in memory concurrently. If not specified\n     * defaults to the specified concurrency limit.\n     * @property {string} [uploadId] If specified attempts to resume a previous upload.\n     * @property {Map} [partsMap] If specified alongside uploadId, attempts to resume a previous upload from the last chunk\n     * specified in partsMap\n     * @experimental\n     */\n    /**\n     * Upload a large file in chunks utilizing parallel upload opertions. If the upload fails, an uploadId and\n     * map containing all the successfully uploaded parts will be returned to the caller. These arguments can be used to\n     * resume the upload.\n     *\n     * @param {string} [filePath] The path of the file to be uploaded\n     * @param {UploadFileInChunksOptions} [options] Configuration options.\n     * @param {MultiPartHelperGenerator} [generator] A function that will return a type that implements the MPU interface. Most users will not need to use this.\n     * @returns {Promise<void>} If successful a promise resolving to void, otherwise a error containing the message, uploadid, and parts map.\n     *\n     * @example\n     * ```\n     * const {Storage} = require('@google-cloud/storage');\n     * const storage = new Storage();\n     * const bucket = storage.bucket('my-bucket');\n     * const transferManager = new TransferManager(bucket);\n     *\n     * //-\n     * // Upload a large file in chunks utilizing parallel operations.\n     * //-\n     * const response = await transferManager.uploadFileInChunks('large-file.txt');\n     * // Your bucket now contains:\n     * // - \"large-file.txt\"\n     * ```\n     *\n     * @experimental\n     */\n    async uploadFileInChunks(filePath, options = {}, generator = defaultMultiPartGenerator) {\n        const chunkSize = options.chunkSizeBytes || UPLOAD_IN_CHUNKS_DEFAULT_CHUNK_SIZE;\n        const limit = pLimit(options.concurrencyLimit || DEFAULT_PARALLEL_CHUNKED_UPLOAD_LIMIT);\n        const maxQueueSize = options.maxQueueSize ||\n            options.concurrencyLimit ||\n            DEFAULT_PARALLEL_CHUNKED_UPLOAD_LIMIT;\n        const fileName = options.uploadName || path.basename(filePath);\n        const mpuHelper = generator(this.bucket, fileName, options.uploadId, options.partsMap);\n        let partNumber = 1;\n        let promises = [];\n        try {\n            if (options.uploadId === undefined) {\n                await mpuHelper.initiateUpload();\n            }\n            const startOrResumptionByte = mpuHelper.partsMap.size * chunkSize;\n            const readStream = (0, fs_1.createReadStream)(filePath, {\n                highWaterMark: chunkSize,\n                start: startOrResumptionByte,\n            });\n            // p-limit only limits the number of running promises. We do not want to hold an entire\n            // large file in memory at once so promises acts a queue that will hold only maxQueueSize in memory.\n            for await (const curChunk of readStream) {\n                if (promises.length >= maxQueueSize) {\n                    await Promise.all(promises);\n                    promises = [];\n                }\n                promises.push(limit(() => mpuHelper.uploadPart(partNumber++, curChunk, options.validation)));\n            }\n            await Promise.all(promises);\n            return await mpuHelper.completeUpload();\n        }\n        catch (e) {\n            throw new MultiPartUploadError(e.message, mpuHelper.uploadId, mpuHelper.partsMap);\n        }\n    }\n    async *getPathsFromDirectory(directory) {\n        const filesAndSubdirectories = await fs_1.promises.readdir(directory, {\n            withFileTypes: true,\n        });\n        for (const curFileOrDirectory of filesAndSubdirectories) {\n            const fullPath = path.join(directory, curFileOrDirectory.name);\n            curFileOrDirectory.isDirectory()\n                ? yield* this.getPathsFromDirectory(fullPath)\n                : yield fullPath;\n        }\n    }\n}\nexports.TransferManager = TransferManager;\n//# sourceMappingURL=transfer-manager.js.map"],"mappings":"AAAA,YAAY;;AACZ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAIA,sBAAsB,GAAI,IAAI,IAAI,IAAI,CAACA,sBAAsB,IAAK,UAAUC,QAAQ,EAAEC,KAAK,EAAEC,IAAI,EAAEC,CAAC,EAAE;EACtG,IAAID,IAAI,KAAK,GAAG,IAAI,CAACC,CAAC,EAAE,MAAM,IAAIC,SAAS,CAAC,+CAA+C,CAAC;EAC5F,IAAI,OAAOH,KAAK,KAAK,UAAU,GAAGD,QAAQ,KAAKC,KAAK,IAAI,CAACE,CAAC,GAAG,CAACF,KAAK,CAACI,GAAG,CAACL,QAAQ,CAAC,EAAE,MAAM,IAAII,SAAS,CAAC,0EAA0E,CAAC;EAClL,OAAOF,IAAI,KAAK,GAAG,GAAGC,CAAC,GAAGD,IAAI,KAAK,GAAG,GAAGC,CAAC,CAACG,IAAI,CAACN,QAAQ,CAAC,GAAGG,CAAC,GAAGA,CAAC,CAACI,KAAK,GAAGN,KAAK,CAACO,GAAG,CAACR,QAAQ,CAAC;AACjG,CAAC;AACD,IAAIS,mCAAmC,EAAEC,6CAA6C;AACtFC,MAAM,CAACC,cAAc,CAACC,OAAO,EAAE,YAAY,EAAE;EAAEN,KAAK,EAAE;AAAK,CAAC,CAAC;AAC7DM,OAAO,CAACC,eAAe,GAAGD,OAAO,CAACE,oBAAoB,GAAG,KAAK,CAAC;AAC/D,MAAMC,MAAM,GAAGC,OAAO,CAAC,SAAS,CAAC;AACjC,MAAMC,IAAI,GAAGD,OAAO,CAAC,MAAM,CAAC;AAC5B,MAAME,MAAM,GAAGF,OAAO,CAAC,QAAQ,CAAC;AAChC,MAAMG,IAAI,GAAGH,OAAO,CAAC,IAAI,CAAC;AAC1B,MAAMI,QAAQ,GAAGJ,OAAO,CAAC,UAAU,CAAC;AACpC,MAAMK,qBAAqB,GAAGL,OAAO,CAAC,qBAAqB,CAAC;AAC5D,MAAMM,iBAAiB,GAAGN,OAAO,CAAC,iBAAiB,CAAC;AACpD,MAAMO,KAAK,GAAGP,OAAO,CAAC,aAAa,CAAC;AACpC,MAAMQ,QAAQ,GAAGR,OAAO,CAAC,QAAQ,CAAC;AAClC;AACA;AACA;AACA;AACA,MAAMS,6BAA6B,GAAG,CAAC;AACvC;AACA;AACA;AACA;AACA,MAAMC,+BAA+B,GAAG,CAAC;AACzC;AACA;AACA;AACA;AACA,MAAMC,uCAAuC,GAAG,CAAC;AACjD;AACA;AACA;AACA;AACA,MAAMC,sCAAsC,GAAG,EAAE,GAAG,IAAI,GAAG,IAAI;AAC/D;AACA;AACA;AACA;AACA,MAAMC,qCAAqC,GAAG,EAAE,GAAG,IAAI,GAAG,IAAI;AAC9D;AACA;AACA;AACA;AACA,MAAMC,mCAAmC,GAAG,EAAE,GAAG,IAAI,GAAG,IAAI;AAC5D;AACA;AACA;AACA;AACA,MAAMC,qCAAqC,GAAG,CAAC;AAC/C,MAAMC,WAAW,GAAG,MAAM;AAC1B,MAAMC,yBAAyB,GAAGA,CAACC,MAAM,EAAEC,QAAQ,EAAEC,QAAQ,EAAEC,QAAQ,KAAK;EACxE,OAAO,IAAIC,wBAAwB,CAACJ,MAAM,EAAEC,QAAQ,EAAEC,QAAQ,EAAEC,QAAQ,CAAC;AAC7E,CAAC;AACD,MAAMvB,oBAAoB,SAASyB,KAAK,CAAC;EACrCC,WAAWA,CAACC,OAAO,EAAEL,QAAQ,EAAEC,QAAQ,EAAE;IACrC,KAAK,CAACI,OAAO,CAAC;IACd,IAAI,CAACL,QAAQ,GAAGA,QAAQ;IACxB,IAAI,CAACC,QAAQ,GAAGA,QAAQ;EAC5B;AACJ;AACAzB,OAAO,CAACE,oBAAoB,GAAGA,oBAAoB;AACnD;AACA;AACA;AACA;AACA;AACA;AACA,MAAMwB,wBAAwB,CAAC;EAC3BE,WAAWA,CAACN,MAAM,EAAEC,QAAQ,EAAEC,QAAQ,EAAEC,QAAQ,EAAE;IAC9C7B,mCAAmC,CAACkC,GAAG,CAAC,IAAI,CAAC;IAC7C,IAAI,CAACC,UAAU,GAAGT,MAAM,CAACU,OAAO,CAACD,UAAU,IAAI,IAAItB,qBAAqB,CAACwB,UAAU,CAAC,CAAC;IACrF,IAAI,CAACT,QAAQ,GAAGA,QAAQ,IAAI,EAAE;IAC9B,IAAI,CAACF,MAAM,GAAGA,MAAM;IACpB,IAAI,CAACC,QAAQ,GAAGA,QAAQ;IACxB;IACA,IAAI,CAACW,OAAO,GAAI,WAAUZ,MAAM,CAACa,IAAK,IAAG,IAAIC,GAAG,CAAC,IAAI,CAACd,MAAM,CAACU,OAAO,CAACK,WAAW,CAAC,CAACC,QAAS,IAAGf,QAAS,EAAC;IACxG,IAAI,CAACgB,UAAU,GAAG,IAAI7B,iBAAiB,CAAC8B,UAAU,CAAC;MAAEC,aAAa,EAAE;IAAO,CAAC,CAAC;IAC7E,IAAI,CAACC,SAAS,GAAG,IAAIhC,iBAAiB,CAACiC,SAAS,CAAC,CAAC;IAClD,IAAI,CAAClB,QAAQ,GAAGA,QAAQ,IAAI,IAAImB,GAAG,CAAC,CAAC;IACrC,IAAI,CAACC,YAAY,GAAG;MAChBC,OAAO,EAAE,IAAI,CAACxB,MAAM,CAACU,OAAO,CAACa,YAAY,CAACE,UAAU;MACpDC,MAAM,EAAE,IAAI,CAAC1B,MAAM,CAACU,OAAO,CAACa,YAAY,CAACI,oBAAoB;MAC7DC,UAAU,EAAE,IAAI,CAAC5B,MAAM,CAACU,OAAO,CAACa,YAAY,CAACM,aAAa,GAAG,IAAI;MACjEC,YAAY,EAAE,IAAI,CAAC9B,MAAM,CAACU,OAAO,CAACa,YAAY,CAACQ,YAAY,GAAG;IAClE,CAAC;EACL;EACA;AACJ;AACA;AACA;AACA;EACI,MAAMC,cAAcA,CAAA,EAAG;IACnB,MAAMC,GAAG,GAAI,GAAE,IAAI,CAACrB,OAAQ,UAAS;IACrC,OAAOvB,KAAK,CAAC,MAAO6C,IAAI,IAAK;MACzB,IAAI;QACA,MAAMC,GAAG,GAAG,MAAM,IAAI,CAAC1B,UAAU,CAAC2B,OAAO,CAAC;UACtCC,MAAM,EAAE,MAAM;UACdJ;QACJ,CAAC,CAAC;QACF,IAAIE,GAAG,CAACG,IAAI,IAAIH,GAAG,CAACG,IAAI,CAACC,KAAK,EAAE;UAC5B,MAAMJ,GAAG,CAACG,IAAI,CAACC,KAAK;QACxB;QACA,MAAMC,SAAS,GAAG,IAAI,CAACpB,SAAS,CAACqB,KAAK,CAACN,GAAG,CAACG,IAAI,CAAC;QAChD,IAAI,CAACpC,QAAQ,GAAGsC,SAAS,CAACE,6BAA6B,CAACC,QAAQ;MACpE,CAAC,CACD,OAAOC,CAAC,EAAE;QACNhF,sBAAsB,CAAC,IAAI,EAAEU,mCAAmC,EAAE,GAAG,EAAEC,6CAA6C,CAAC,CAACJ,IAAI,CAAC,IAAI,EAAEyE,CAAC,EAAEV,IAAI,CAAC;MAC7I;IACJ,CAAC,EAAE,IAAI,CAACX,YAAY,CAAC;EACzB;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMsB,UAAUA,CAACC,UAAU,EAAEC,KAAK,EAAEC,UAAU,EAAE;IAC5C,MAAMf,GAAG,GAAI,GAAE,IAAI,CAACrB,OAAQ,eAAckC,UAAW,aAAY,IAAI,CAAC5C,QAAS,EAAC;IAChF,IAAI+C,OAAO,GAAG,CAAC,CAAC;IAChB,IAAID,UAAU,KAAK,KAAK,EAAE;MACtB,MAAME,IAAI,GAAG,CAAC,CAAC,EAAE5D,QAAQ,CAAC6D,UAAU,EAAE,KAAK,CAAC,CAACC,MAAM,CAACL,KAAK,CAAC,CAACM,MAAM,CAAC,QAAQ,CAAC;MAC3EJ,OAAO,GAAG;QACN,aAAa,EAAEC;MACnB,CAAC;IACL;IACA,OAAO7D,KAAK,CAAC,MAAO6C,IAAI,IAAK;MACzB,IAAI;QACA,MAAMC,GAAG,GAAG,MAAM,IAAI,CAAC1B,UAAU,CAAC2B,OAAO,CAAC;UACtCH,GAAG;UACHI,MAAM,EAAE,KAAK;UACbiB,IAAI,EAAEP,KAAK;UACXE;QACJ,CAAC,CAAC;QACF,IAAId,GAAG,CAACG,IAAI,IAAIH,GAAG,CAACG,IAAI,CAACC,KAAK,EAAE;UAC5B,MAAMJ,GAAG,CAACG,IAAI,CAACC,KAAK;QACxB;QACA,IAAI,CAACpC,QAAQ,CAACoD,GAAG,CAACT,UAAU,EAAEX,GAAG,CAACc,OAAO,CAAC,MAAM,CAAC,CAAC;MACtD,CAAC,CACD,OAAOL,CAAC,EAAE;QACNhF,sBAAsB,CAAC,IAAI,EAAEU,mCAAmC,EAAE,GAAG,EAAEC,6CAA6C,CAAC,CAACJ,IAAI,CAAC,IAAI,EAAEyE,CAAC,EAAEV,IAAI,CAAC;MAC7I;IACJ,CAAC,EAAE,IAAI,CAACX,YAAY,CAAC;EACzB;EACA;AACJ;AACA;AACA;AACA;EACI,MAAMiC,cAAcA,CAAA,EAAG;IACnB,MAAMvB,GAAG,GAAI,GAAE,IAAI,CAACrB,OAAQ,aAAY,IAAI,CAACV,QAAS,EAAC;IACvD,MAAMuD,SAAS,GAAG,IAAInC,GAAG,CAAC,CAAC,GAAG,IAAI,CAACnB,QAAQ,CAACuD,OAAO,CAAC,CAAC,CAAC,CAACC,IAAI,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,CAAC,CAAC,CAAC,GAAGC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IACnF,MAAMC,KAAK,GAAG,EAAE;IAChB,KAAK,MAAMC,KAAK,IAAIN,SAAS,CAACC,OAAO,CAAC,CAAC,EAAE;MACrCI,KAAK,CAACE,IAAI,CAAC;QAAEC,UAAU,EAAEF,KAAK,CAAC,CAAC,CAAC;QAAEG,IAAI,EAAEH,KAAK,CAAC,CAAC;MAAE,CAAC,CAAC;IACxD;IACA,MAAMT,IAAI,GAAI,4BAA2B,IAAI,CAACrC,UAAU,CAACkD,KAAK,CAACL,KAAK,CAAE,4BAA2B;IACjG,OAAOzE,KAAK,CAAC,MAAO6C,IAAI,IAAK;MACzB,IAAI;QACA,MAAMC,GAAG,GAAG,MAAM,IAAI,CAAC1B,UAAU,CAAC2B,OAAO,CAAC;UACtCH,GAAG;UACHI,MAAM,EAAE,MAAM;UACdiB;QACJ,CAAC,CAAC;QACF,IAAInB,GAAG,CAACG,IAAI,IAAIH,GAAG,CAACG,IAAI,CAACC,KAAK,EAAE;UAC5B,MAAMJ,GAAG,CAACG,IAAI,CAACC,KAAK;QACxB;QACA,OAAOJ,GAAG;MACd,CAAC,CACD,OAAOS,CAAC,EAAE;QACNhF,sBAAsB,CAAC,IAAI,EAAEU,mCAAmC,EAAE,GAAG,EAAEC,6CAA6C,CAAC,CAACJ,IAAI,CAAC,IAAI,EAAEyE,CAAC,EAAEV,IAAI,CAAC;QACzI;MACJ;IACJ,CAAC,EAAE,IAAI,CAACX,YAAY,CAAC;EACzB;AACJ;AACAjD,mCAAmC,GAAG,IAAI8F,OAAO,CAAC,CAAC,EAAE7F,6CAA6C,GAAG,SAASA,6CAA6CA,CAAC8F,GAAG,EAAEnC,IAAI,EAAE;EACnK,IAAI,IAAI,CAAClC,MAAM,CAACU,OAAO,CAACa,YAAY,CAAC+C,SAAS,IAC1C,IAAI,CAACtE,MAAM,CAACU,OAAO,CAACa,YAAY,CAACgD,gBAAgB,CAACF,GAAG,CAAC,EAAE;IACxD,MAAMA,GAAG;EACb,CAAC,MACI;IACDnC,IAAI,CAACmC,GAAG,CAAC;EACb;AACJ,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM1F,eAAe,CAAC;EAClB2B,WAAWA,CAACN,MAAM,EAAE;IAChB,IAAI,CAACA,MAAM,GAAGA,MAAM;EACxB;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMwE,eAAeA,CAACC,oBAAoB,EAAEC,OAAO,GAAG,CAAC,CAAC,EAAE;IACtD,IAAIC,EAAE;IACN,IAAID,OAAO,CAACE,YAAY,KAAK,CAACD,EAAE,GAAGD,OAAO,CAACG,kBAAkB,MAAM,IAAI,IAAIF,EAAE,KAAK,KAAK,CAAC,GAAG,KAAK,CAAC,GAAGA,EAAE,CAACG,gBAAgB,CAAC,EAAE;MACtHJ,OAAO,CAACG,kBAAkB,CAACC,gBAAgB,CAACC,iBAAiB,GAAG,CAAC;IACrE,CAAC,MACI,IAAIL,OAAO,CAACE,YAAY,IACzBF,OAAO,CAACG,kBAAkB,KAAKG,SAAS,EAAE;MAC1CN,OAAO,CAACG,kBAAkB,GAAG;QACzBC,gBAAgB,EAAE;UACdC,iBAAiB,EAAE;QACvB;MACJ,CAAC;IACL;IACA,MAAME,KAAK,GAAGpG,MAAM,CAAC6F,OAAO,CAACQ,gBAAgB,IAAI3F,6BAA6B,CAAC;IAC/E,MAAM4F,QAAQ,GAAG,EAAE;IACnB,IAAIC,QAAQ,GAAG,EAAE;IACjB,IAAI,CAACC,KAAK,CAACC,OAAO,CAACb,oBAAoB,CAAC,EAAE;MACtC,WAAW,MAAMc,OAAO,IAAI,IAAI,CAACC,qBAAqB,CAACf,oBAAoB,CAAC,EAAE;QAC1EW,QAAQ,CAACpB,IAAI,CAACuB,OAAO,CAAC;MAC1B;IACJ,CAAC,MACI;MACDH,QAAQ,GAAGX,oBAAoB;IACnC;IACA,KAAK,MAAMgB,QAAQ,IAAIL,QAAQ,EAAE;MAC7B,MAAMM,IAAI,GAAG,MAAMzG,IAAI,CAACkG,QAAQ,CAACQ,KAAK,CAACF,QAAQ,CAAC;MAChD,IAAIC,IAAI,CAACE,WAAW,CAAC,CAAC,EAAE;QACpB;MACJ;MACA,MAAMC,sBAAsB,GAAG7G,MAAM,CAAC,IAAI,EAAE,CAAC,CAAC,EAAE0F,OAAO,CAACG,kBAAkB,CAAC;MAC3EgB,sBAAsB,CAACC,WAAW,GAAGL,QAAQ;MAC7C,IAAIf,OAAO,CAACqB,MAAM,EAAE;QAChBF,sBAAsB,CAACC,WAAW,GAAG/G,IAAI,CAACiH,IAAI,CAACtB,OAAO,CAACqB,MAAM,EAAEF,sBAAsB,CAACC,WAAW,CAAC;MACtG;MACAX,QAAQ,CAACnB,IAAI,CAACiB,KAAK,CAAC,MAAM,IAAI,CAACjF,MAAM,CAACiG,MAAM,CAACR,QAAQ,EAAEI,sBAAsB,CAAC,CAAC,CAAC;IACpF;IACA,OAAOK,OAAO,CAACC,GAAG,CAAChB,QAAQ,CAAC;EAChC;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMiB,iBAAiBA,CAACC,aAAa,EAAE3B,OAAO,GAAG,CAAC,CAAC,EAAE;IACjD,MAAMO,KAAK,GAAGpG,MAAM,CAAC6F,OAAO,CAACQ,gBAAgB,IAAI1F,+BAA+B,CAAC;IACjF,MAAM2F,QAAQ,GAAG,EAAE;IACnB,IAAImB,KAAK,GAAG,EAAE;IACd,IAAI,CAACjB,KAAK,CAACC,OAAO,CAACe,aAAa,CAAC,EAAE;MAC/B,MAAME,cAAc,GAAG,MAAM,IAAI,CAACvG,MAAM,CAACwG,QAAQ,CAAC;QAC9CT,MAAM,EAAEM;MACZ,CAAC,CAAC;MACFC,KAAK,GAAGC,cAAc,CAAC,CAAC,CAAC;IAC7B,CAAC,MACI;MACDD,KAAK,GAAGD,aAAa,CAACI,GAAG,CAACC,OAAO,IAAI;QACjC,IAAI,OAAOA,OAAO,KAAK,QAAQ,EAAE;UAC7B,OAAO,IAAI,CAAC1G,MAAM,CAAC2G,IAAI,CAACD,OAAO,CAAC;QACpC;QACA,OAAOA,OAAO;MAClB,CAAC,CAAC;IACN;IACA,MAAME,gBAAgB,GAAGlC,OAAO,CAACmC,WAAW,GACrC,IAAGnC,OAAO,CAACmC,WAAY,EAAC,GACzB/G,WAAW;IACjB,MAAMgH,KAAK,GAAG,IAAIC,MAAM,CAACH,gBAAgB,EAAE,GAAG,CAAC;IAC/C,KAAK,MAAMD,IAAI,IAAIL,KAAK,EAAE;MACtB,MAAMT,sBAAsB,GAAG7G,MAAM,CAAC,IAAI,EAAE,CAAC,CAAC,EAAE0F,OAAO,CAACG,kBAAkB,CAAC;MAC3E,IAAIH,OAAO,CAACqB,MAAM,EAAE;QAChBF,sBAAsB,CAACC,WAAW,GAAG/G,IAAI,CAACiH,IAAI,CAACtB,OAAO,CAACqB,MAAM,IAAI,EAAE,EAAEF,sBAAsB,CAACC,WAAW,IAAI,EAAE,EAAEa,IAAI,CAAC9F,IAAI,CAAC;MAC7H;MACA,IAAI6D,OAAO,CAACmC,WAAW,EAAE;QACrBhB,sBAAsB,CAACC,WAAW,GAAGa,IAAI,CAAC9F,IAAI,CAACmG,OAAO,CAACF,KAAK,EAAE,EAAE,CAAC;MACrE;MACA3B,QAAQ,CAACnB,IAAI,CAACiB,KAAK,CAAC,MAAM0B,IAAI,CAACM,QAAQ,CAACpB,sBAAsB,CAAC,CAAC,CAAC;IACrE;IACA,OAAOK,OAAO,CAACC,GAAG,CAAChB,QAAQ,CAAC;EAChC;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAM+B,oBAAoBA,CAACC,UAAU,EAAEzC,OAAO,GAAG,CAAC,CAAC,EAAE;IACjD,IAAI0C,SAAS,GAAG1C,OAAO,CAAC2C,cAAc,IAAI1H,qCAAqC;IAC/E,IAAIsF,KAAK,GAAGpG,MAAM,CAAC6F,OAAO,CAACQ,gBAAgB,IAAIzF,uCAAuC,CAAC;IACvF,MAAM0F,QAAQ,GAAG,EAAE;IACnB,MAAMwB,IAAI,GAAG,OAAOQ,UAAU,KAAK,QAAQ,GACrC,IAAI,CAACnH,MAAM,CAAC2G,IAAI,CAACQ,UAAU,CAAC,GAC5BA,UAAU;IAChB,MAAMG,QAAQ,GAAG,MAAMX,IAAI,CAACtI,GAAG,CAAC,CAAC;IACjC,MAAMkJ,IAAI,GAAGC,QAAQ,CAACF,QAAQ,CAAC,CAAC,CAAC,CAACG,QAAQ,CAACF,IAAI,CAAC;IAChD;IACA,IAAIA,IAAI,GAAG7H,sCAAsC,EAAE;MAC/CuF,KAAK,GAAGpG,MAAM,CAAC,CAAC,CAAC;MACjBuI,SAAS,GAAGG,IAAI;IACpB;IACA,IAAIG,KAAK,GAAG,CAAC;IACb,MAAMjC,QAAQ,GAAGf,OAAO,CAACoB,WAAW,IAAI/G,IAAI,CAAC4I,QAAQ,CAAChB,IAAI,CAAC9F,IAAI,CAAC;IAChE,MAAM+G,WAAW,GAAG,MAAM3I,IAAI,CAACkG,QAAQ,CAAC0C,IAAI,CAACpC,QAAQ,EAAE,IAAI,CAAC;IAC5D,OAAOiC,KAAK,GAAGH,IAAI,EAAE;MACjB,MAAMO,UAAU,GAAGJ,KAAK;MACxB,IAAIK,QAAQ,GAAGL,KAAK,GAAGN,SAAS,GAAG,CAAC;MACpCW,QAAQ,GAAGA,QAAQ,GAAGR,IAAI,GAAGA,IAAI,GAAGQ,QAAQ;MAC5C5C,QAAQ,CAACnB,IAAI,CAACiB,KAAK,CAAC,MAAM0B,IAAI,CAACM,QAAQ,CAAC;QAAES,KAAK,EAAEI,UAAU;QAAEE,GAAG,EAAED;MAAS,CAAC,CAAC,CAACE,IAAI,CAACC,IAAI,IAAI;QACvF,OAAON,WAAW,CAACO,KAAK,CAACD,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,EAAEA,IAAI,CAAC,CAAC,CAAC,CAACE,MAAM,EAAEN,UAAU,CAAC;MACpE,CAAC,CAAC,CAAC,CAAC;MACJJ,KAAK,IAAIN,SAAS;IACtB;IACA,OAAO,IAAIlB,OAAO,CAAC,CAACmC,OAAO,EAAEC,MAAM,KAAK;MACpC,IAAIC,OAAO;MACXrC,OAAO,CAACC,GAAG,CAAChB,QAAQ,CAAC,CAChB8C,IAAI,CAAC3F,IAAI,IAAI;QACdiG,OAAO,GAAGjG,IAAI,CAACmE,GAAG,CAAC+B,MAAM,IAAIA,MAAM,CAACC,MAAM,CAAC;QAC3C,IAAI/D,OAAO,CAAC1B,UAAU,KAAK,QAAQ,EAAE;UACjC,OAAO9D,QAAQ,CAACwJ,MAAM,CAACC,QAAQ,CAAClD,QAAQ,CAAC;QAC7C;QACA;MACJ,CAAC,CAAC,CACGwC,IAAI,CAAC,MAAM;QACZI,OAAO,CAACE,OAAO,CAAC;MACpB,CAAC,CAAC,CACGK,KAAK,CAAChG,CAAC,IAAI;QACZ0F,MAAM,CAAC1F,CAAC,CAAC;MACb,CAAC,CAAC,CACGiG,OAAO,CAAC,MAAM;QACfjB,WAAW,CAACkB,KAAK,CAAC,CAAC;MACvB,CAAC,CAAC;IACN,CAAC,CAAC;EACN;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMC,kBAAkBA,CAACtD,QAAQ,EAAEf,OAAO,GAAG,CAAC,CAAC,EAAEsE,SAAS,GAAGjJ,yBAAyB,EAAE;IACpF,MAAMqH,SAAS,GAAG1C,OAAO,CAAC2C,cAAc,IAAIzH,mCAAmC;IAC/E,MAAMqF,KAAK,GAAGpG,MAAM,CAAC6F,OAAO,CAACQ,gBAAgB,IAAIrF,qCAAqC,CAAC;IACvF,MAAMoJ,YAAY,GAAGvE,OAAO,CAACuE,YAAY,IACrCvE,OAAO,CAACQ,gBAAgB,IACxBrF,qCAAqC;IACzC,MAAMI,QAAQ,GAAGyE,OAAO,CAACwE,UAAU,IAAInK,IAAI,CAAC4I,QAAQ,CAAClC,QAAQ,CAAC;IAC9D,MAAM0D,SAAS,GAAGH,SAAS,CAAC,IAAI,CAAChJ,MAAM,EAAEC,QAAQ,EAAEyE,OAAO,CAACxE,QAAQ,EAAEwE,OAAO,CAACvE,QAAQ,CAAC;IACtF,IAAI2C,UAAU,GAAG,CAAC;IAClB,IAAIqC,QAAQ,GAAG,EAAE;IACjB,IAAI;MACA,IAAIT,OAAO,CAACxE,QAAQ,KAAK8E,SAAS,EAAE;QAChC,MAAMmE,SAAS,CAACnH,cAAc,CAAC,CAAC;MACpC;MACA,MAAMoH,qBAAqB,GAAGD,SAAS,CAAChJ,QAAQ,CAACoH,IAAI,GAAGH,SAAS;MACjE,MAAMiC,UAAU,GAAG,CAAC,CAAC,EAAEpK,IAAI,CAACqK,gBAAgB,EAAE7D,QAAQ,EAAE;QACpD8D,aAAa,EAAEnC,SAAS;QACxBM,KAAK,EAAE0B;MACX,CAAC,CAAC;MACF;MACA;MACA,WAAW,MAAMI,QAAQ,IAAIH,UAAU,EAAE;QACrC,IAAIlE,QAAQ,CAACiD,MAAM,IAAIa,YAAY,EAAE;UACjC,MAAM/C,OAAO,CAACC,GAAG,CAAChB,QAAQ,CAAC;UAC3BA,QAAQ,GAAG,EAAE;QACjB;QACAA,QAAQ,CAACnB,IAAI,CAACiB,KAAK,CAAC,MAAMkE,SAAS,CAACtG,UAAU,CAACC,UAAU,EAAE,EAAE0G,QAAQ,EAAE9E,OAAO,CAAC1B,UAAU,CAAC,CAAC,CAAC;MAChG;MACA,MAAMkD,OAAO,CAACC,GAAG,CAAChB,QAAQ,CAAC;MAC3B,OAAO,MAAMgE,SAAS,CAAC3F,cAAc,CAAC,CAAC;IAC3C,CAAC,CACD,OAAOZ,CAAC,EAAE;MACN,MAAM,IAAIhE,oBAAoB,CAACgE,CAAC,CAACrC,OAAO,EAAE4I,SAAS,CAACjJ,QAAQ,EAAEiJ,SAAS,CAAChJ,QAAQ,CAAC;IACrF;EACJ;EACA,OAAOqF,qBAAqBA,CAACiE,SAAS,EAAE;IACpC,MAAMC,sBAAsB,GAAG,MAAMzK,IAAI,CAACkG,QAAQ,CAACwE,OAAO,CAACF,SAAS,EAAE;MAClEG,aAAa,EAAE;IACnB,CAAC,CAAC;IACF,KAAK,MAAMC,kBAAkB,IAAIH,sBAAsB,EAAE;MACrD,MAAMI,QAAQ,GAAG/K,IAAI,CAACiH,IAAI,CAACyD,SAAS,EAAEI,kBAAkB,CAAChJ,IAAI,CAAC;MAC9DgJ,kBAAkB,CAACjE,WAAW,CAAC,CAAC,GAC1B,OAAO,IAAI,CAACJ,qBAAqB,CAACsE,QAAQ,CAAC,GAC3C,MAAMA,QAAQ;IACxB;EACJ;AACJ;AACApL,OAAO,CAACC,eAAe,GAAGA,eAAe"},"metadata":{},"sourceType":"script","externalDependencies":[]}